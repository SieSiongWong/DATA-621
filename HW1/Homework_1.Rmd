---
title: 'Homework #1'
author: "Critical Thinking Group #2"
date: "9/6/2021"
output: html_document
---

# Load R Packages

```{r Libraries}

library(dplyr)
library(ggplot2)
library(tidyr)
library(data.table)
library(VIM)
library(mice)
library(Rcpp)
library(corrplot)
library(stats)
library(rstatix)
library(DataExplorer)

```

# Load the Data

```{r Data Importation}

training <- read.csv("https://raw.githubusercontent.com/SieSiongWong/DATA-621/main/HW1/moneyball-training-data.csv", header=TRUE, sep=",")

evaluation <- read.csv("https://raw.githubusercontent.com/SieSiongWong/DATA-621/main/HW1/moneyball-evaluation-data.csv", header=TRUE, sep=",")

```

# Introduction



# Data Exploration

With regards to our training set, we can see quite a few of the variables have missing values. The two primary methods to handle these missing values are imputation or data removal.  There are couple options to deal with these missing values.

- remove rows containing missing values
- replace missing values with an average value
- replace missing value with a median value

From our missing values chart, we can see that there are 91% missing values in TEAM_BATTING_HBP variable, 34% missing values in TEAM_BASERUN_CS and so on.

```{r Data Exploration}

# First 10 rows of DataSets

head(training,10)
head(evaluation,10)

# Last 10 rows of DataSets

tail(training,10)
tail(evaluation,10)

# Dimensions

dim(training)
dim(evaluation)

# Column names

colnames(training)
colnames(evaluation)

# Total NAs
sum(is.na(training))
sum(is.na(evaluation))

# Graphical Exploration of DataSets

## Densities

plot_density(training)
plot_density(evaluation)

## Correlations

plot_correlation(training)
plot_correlation(evaluation)

## Plot Missing Data

plot_missing(training)
plot_missing(evaluation)

# Dataset Summaries

summary(training)
summary(evaluation)

# Std Devs assuming NAs are filtered out

training %>% gather(variable, value) %>% filter(!is.na(value)) %>% group_by(variable) %>% summarize(sd = sd(value))

evaluation %>% gather(variable, value) %>% filter(!is.na(value)) %>% group_by(variable) %>% summarize(sd = sd(value))

```


# Data Transformation and Non Visual Data

```{r Data Transformation}

# Remove index column
train_df <- training %>% subset(select=-INDEX)

# Rename columns
setnames(train_df, old = c('TARGET_WINS','TEAM_BATTING_H','TEAM_BATTING_2B','TEAM_BATTING_3B','TEAM_BATTING_HR','TEAM_BATTING_BB','TEAM_BATTING_SO','TEAM_BASERUN_SB','TEAM_BASERUN_CS','TEAM_BATTING_HBP','TEAM_PITCHING_H','TEAM_PITCHING_HR','TEAM_PITCHING_BB','TEAM_PITCHING_SO','TEAM_FIELDING_E','TEAM_FIELDING_DP'), new = c('wins','bat_H','bat_2b','bat_3b','bat_hr','bat_bb','bat_so','brun_sb','brun_cs','bat_hbp','pitch_h','pitch_hr','picth_bb','pitch_so','field_e','field_dp'))

```


```{r Non Visual Representation of NAs}
NA_Buckets <- train_df %>% 
  gather(variable, value) %>%
  filter(is.na(value)) %>%
  group_by(variable) %>%
  tally() %>%
  mutate(percent = n / nrow(training) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>%
  rename(`Variable Missing Data` = variable,
         `Number of Records` = n,
         `Share of Total` = percent)

## Non Visual Breakout of Missing Data

print(NA_Buckets)

# Visual Representation of Zero Values
Zero_Value_Buckets <- train_df %>% 
  gather(variable, value) %>%
  filter(value == 0) %>%
  group_by(variable) %>%
  tally() %>%
  mutate(percent = n / nrow(training) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>%
  rename(`Variable With Zeros` = variable,
         `Number of Records` = n,
         `Share of Total` = percent)

print(Zero_Value_Buckets)

## Top 4 Require Further Inquiry

top_zeros = head(Zero_Value_Buckets,4)

## Only requires two dataframes as zero entries overlap between top 4 variables

train_df %>% filter(bat_so==0)
train_df %>% filter(pitch_hr==0)

```

# Data Preparation

```{r data preparation}
# Drop the bat_hbp variable
train_df <- train_df %>% select(-bat_hbp)

# Using the mice package to impute the missing values
imputed_data <- mice(train_df, m=5, maxit=30, method='pmm', seed=321)


# Select complete imputed data set number 2
complete_train_data <- complete(imputed_data,2)
complete_train_data

# Density plot of completed dataset

plot_density(complete_train_data)
```

Check correlation between each variable

```{r Correlation Test}

complete_train_data %>% dplyr::select(wins:field_dp) %>% 
  cor() %>% as.matrix() %>% 
  corrplot(type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

```
Correlation Test

```{r}

complete_train_data %>% cor_test(wins, method="pearson", conf.level = 0.95) %>% arrange(desc(cor))

#Correlation Visualization

corr_complete_data_viz <- complete_train_data %>%
  gather(variable, value, -wins) %>%
  ggplot(., aes(value, wins)) + 
  geom_point(color="#69b3a2") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")

corr_complete_data_viz
```

# Build Models


```{r LM}
# Saturated linear regression model
r.squared <- c()
sat_lm <- lm(wins ~ ., data = complete_train_data)
summary(sat_lm)
r.squared['full'] = summary(sat_lm)$adj.r.squared
```

We can use backward elimination method to eliminate variables do not contribute to the regression equation.

From the summary, we can see these 3 variables pitch_hr,picth_bb,brun_cs which p-value are statistically insignificant. 

```{r drop insignificant variables}
# Test whether pitch_hr, picth_bb, brun_cs can be dropped
lm <- lm(wins ~ ., data = subset(complete_train_data, select=-c(pitch_hr,picth_bb,brun_cs)))
anova(lm, sat_lm)
```

All the 3 variables can be dropped as its p-value of F-test is 0.74. The null hypothesis cannot be rejected.

```{r permutation test}
# Normal theory-based p-value
lms <- summary(lm)
format.pval(1 - pf(lms$fstatistic[1],lms$fstatistic[2],lms$fstatistic[3]))

# Permutation test based p-value
nreps <- 4000
set.seed(123)
fstats <- numeric(nreps)
for(i in 1:nreps){
  lmods <- lm(sample(wins) ~ ., data = subset(complete_train_data,select=-c(pitch_hr,picth_bb,brun_cs)))
  fstats[i] <- summary(lmods)$fstat[1]
}

format.pval(mean(fstats > lms$fstat[1]))
```

We can see the permutation test result similar to the normal theory-based value of 2.22e-16. Also, the p-value is less than the significance level, we can reject the null hypothesis. This indicates the imputed training data set provides sufficient evidence to conclude that regression model fits the data better than the model with no predictor variables.

Check the linear model summary to determine which variables can also be dropped.

```{r build 3 models}
summary(lm)
r.squared['lm'] = summary(lm)$adj.r.squared
```

By looking at the correlation visualization created in above and also the p-values from the summary, below variables we think can be removed to build second model.
- bat_2b
- bat_3b
- pitch_so

```{r 2nd model}
lm_2 <- lm(wins ~ ., data = subset(complete_train_data, select=-c(pitch_hr,picth_bb,brun_cs,bat_2b,bat_3b,pitch_so)))
summary(lm_2)
r.squared['lm_2'] = summary(lm_2)$adj.r.squared
```
As bat_bb variable p-value now is highest in the list, we remove this variable to build third model.

```{r 3rd model}
lm_3 <- lm(wins ~ ., data = subset(complete_train_data, select=-c(pitch_hr,picth_bb,brun_cs,bat_2b,bat_3b,pitch_so,bat_bb)))
summary(lm_3)
r.squared['lm_3'] = summary(lm_3)$adj.r.squared
r.squared
```

Evaluate the best adjusted R-squared mode through residuals plot and QQ plot.

```{r evaluate}
par(mfrow=c(2,2))

# Residuals plot
plot(lm$fitted.values, lm$residuals, 
     xlab='Fitted Values', ylab='Residuals')
abline(h = 0, lty = 3, col="blue")
abline(h = 40, lty = 3, col="red")
abline(h = -40, lty = 3, col="red")

# Histogram plot
hist(lm$residuals, xlab="Residuals")

# QQ plot
qqnorm(lm$residuals)
qqline(lm$residuals)

# Check for nonconstant variances: H0: Constant Variance, HA: Nonconstant Variance
# P-value of .011 <.05, reject null hypothesis for alternative indicating Nonconstant Variance
summary (lm (abs (lm$residuals) ~ lm$fitted.values))

# Test Residuals for Normality
# H0: Residuals are normally distributed, HA: Residuals are Not Normally Distributed
# P-value <.05 so we reject the null hypothesis in favor of the alternative indicating
# Residuals are not normally distributed

shapiro.test(lm$residuals)

```

From the residuals, histogram, and qq plot, we can see this model meets the 3 conditions (linearity, nearly normal residuals, constant variability) of least squares regression.

# Select Models


Evaluate the best model through residuals plot and QQ plot again but this time we're using evaluation data set. 

```{r}

par(mfrow=c(2,2))

# Residuals plot
plot(lm_ev$fitted.values, lm_ev$residuals, 
     xlab='Fitted Values', ylab='Residuals')
abline(h = 0, lty = 3, col="blue")
abline(h = 2e+5, lty = 3, col="red")
abline(h = -2e+5, lty = 3, col="red")

# Histogram plot
hist(lm_ev$residuals, xlab="Residuals")

qqnorm(lm_ev$residuals)
qqline(lm_ev$residuals)

```

We can use Box-Cox transformation to transform non-normal dependent variable (wins) into a normal shape.

```{r}



```
