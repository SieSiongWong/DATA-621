---
title: 'Homework #1'
author: "Critical Thinking Group #2"
date: "9/6/2021"
output: html_document
---

# Load R Packages

```{r Libraries}

library(dplyr)
library(ggplot2)
library(tidyr)
library(data.table)
library(VIM)
library(mice)
library(Rcpp)
library(corrplot)
library(stats)
library(rstatix)
library(DataExplorer)

```

# Load the Data

```{r Data Importation}

training <- read.csv("https://raw.githubusercontent.com/SieSiongWong/DATA-621/main/HW1/moneyball-training-data.csv", header=TRUE, sep=",")

evaluation <- read.csv("https://raw.githubusercontent.com/SieSiongWong/DATA-621/main/HW1/moneyball-evaluation-data.csv", header=TRUE, sep=",")

```

# Introduction



# Data Exploration

With regards to our training set, we can see quite a few of the variables have missing values. The two primary methods to handle these missing values are imputation or data removal.  There are couple options to deal with these missing values.

- remove rows containing missing values
- replace missing values with an average value
- replace missing value with a median value

From our missing values chart, we can see that there are 91% missing values in TEAM_BATTING_HBP variable, 34% missing values in TEAM_BASERUN_CS and so on.

```{r Data Exploration}

# First 10 rows of DataSets

head(training,10)
head(evaluation,10)

# Last 10 rows of DataSets

tail(training,10)
tail(evaluation,10)

# Dimensions

dim(training)
dim(evaluation)

# Column names

colnames(training)
colnames(evaluation)

# Total NAs
sum(is.na(training))
sum(is.na(evaluation))

# Graphical Exploration of DataSets

## Densities

plot_density(training)
plot_density(evaluation)

## Correlations

plot_correlation(training)
plot_correlation(evaluation)

## Plot Missing Data

plot_missing(training)
plot_missing(evaluation)


```


# Data Transformation and Non Visual Data

```{r Data Transformation}

# Remove index column
train_df <- training %>% subset(select=-INDEX)

# Rename columns
setnames(train_df, old = c('TARGET_WINS','TEAM_BATTING_H','TEAM_BATTING_2B','TEAM_BATTING_3B','TEAM_BATTING_HR','TEAM_BATTING_BB','TEAM_BATTING_SO','TEAM_BASERUN_SB','TEAM_BASERUN_CS','TEAM_BATTING_HBP','TEAM_PITCHING_H','TEAM_PITCHING_HR','TEAM_PITCHING_BB','TEAM_PITCHING_SO','TEAM_FIELDING_E','TEAM_FIELDING_DP'), new = c('wins','bat_H','bat_2b','bat_3b','bat_hr','bat_bb','bat_so','brun_sb','brun_cs','bat_hbp','pitch_h','pitch_hr','picth_bb','pitch_so','field_e','field_dp'))

```


```{r Non Visual Representation of NAs}
NA_Buckets <- train_df %>% 
  gather(variable, value) %>%
  filter(is.na(value)) %>%
  group_by(variable) %>%
  tally() %>%
  mutate(percent = n / nrow(training) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>%
  rename(`Variable Missing Data` = variable,
         `Number of Records` = n,
         `Share of Total` = percent)

## Non Visual Breakout of Missing Data

print(NA_Buckets)

# Visual Representation of Zero Values
Zero_Value_Buckets <- train_df %>% 
  gather(variable, value) %>%
  filter(value == 0) %>%
  group_by(variable) %>%
  tally() %>%
  mutate(percent = n / nrow(training) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>%
  rename(`Variable With Zeros` = variable,
         `Number of Records` = n,
         `Share of Total` = percent)

print(Zero_Value_Buckets)

## Top 4 Require Further Inquiry

top_zeros = head(Zero_Value_Buckets,4)

## Only requires two dataframes as zero entries overlap between top 4 variables

train_df %>% filter(bat_so==0)
train_df %>% filter(pitch_hr==0)

```

# Data Preparation

```{r Data Preparation}

# Using the mice package to impute the missing values
imputed_data <- mice(train_df, m=5, maxit=30, method='pmm', seed=321)

str(imputed_data)
# Select complete imputed data set number 2
complete_train_data <- complete(imputed_data,2)
complete_train_data

# Density plot of completed dataset

plot_density(complete_train_data)
```

Check correlation between each variable

```{r Correlation Test}

complete_train_data %>% dplyr::select(wins:field_dp) %>% 
  cor() %>% as.matrix() %>% 
  corrplot(type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

```
Correlation Test

```{r}

complete_train_data %>% cor_test(wins, method="pearson", conf.level = 0.95)

#Correlation Visualization

corr_complete_data_viz <- complete_train_data %>%
  gather(variable, value, -wins) %>%
  ggplot(., aes(value, wins)) + 
  geom_point(color="#69b3a2") + 
  geom_smooth(method = "lm", se = FALSE, color = "black") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = "Wins")

corr_complete_data_viz
```

# Build Models


```{r LM}

sat_lm <- lm(wins ~ ., data = complete_train_data)

summary(sat_lm)


```

We can use backward elimination method to eliminate variables do not contribute to the regression equation.

```{r}




```


Evaluate the each of the 3 models through residuals plot and QQ plot.

```{r}

par(mfrow=c(2,2))

# Residuals plot
plot(mlm_2$fitted.values, mlm_2$residuals, 
     xlab='Fitted Values', ylab='Residuals')
abline(h = 0, lty = 3, col="blue")
abline(h = 2e+5, lty = 3, col="red")
abline(h = -2e+5, lty = 3, col="red")

# Histogram plot
hist(mlm_2$residuals, xlab="Residuals")

qqnorm(mlm_2$residuals)
qqline(mlm_2$residuals)

```


# Select Models


Evaluate the best model through residuals plot and QQ plot again but this time we're using evaluation data set. 

```{r}

par(mfrow=c(2,2))

# Residuals plot
plot(mlm_2$fitted.values, mlm_2$residuals, 
     xlab='Fitted Values', ylab='Residuals')
abline(h = 0, lty = 3, col="blue")
abline(h = 2e+5, lty = 3, col="red")
abline(h = -2e+5, lty = 3, col="red")

# Histogram plot
hist(mlm_2$residuals, xlab="Residuals")

qqnorm(mlm_2$residuals)
qqline(mlm_2$residuals)

```

We can use Box-Cox transformation to transform non-normal dependent variable (wins) into a normal shape.

```{r}



```
