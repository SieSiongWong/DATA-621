---
title: 'Homework #1 V2'
author: "Sie Siong Wong"
date: "9/17/2021"
output: html_document
---

# Load R Packages

```{r warning=FALSE, message=FALSE}

library(dplyr)
library(ggplot2)
library(tidyr)
library(data.table)
library(VIM)
library(mice)
library(Rcpp)
library(corrplot)
library(stats)
library(rstatix)
library(DataExplorer)
library(caret)
library(psych)
library(janitor)
```

# Load the Data

```{r Data Importation}

training <- read.csv("https://raw.githubusercontent.com/SieSiongWong/DATA-621/main/HW1/moneyball-training-data.csv", header=TRUE, sep=",")

evaluation <- read.csv("https://raw.githubusercontent.com/SieSiongWong/DATA-621/main/HW1/moneyball-evaluation-data.csv", header=TRUE, sep=",")

```

# Data Exploration

Data set structure

```{r}
str(training)
str(evaluation)
```
Given training data set has 17 variables and 2276 observations while evaluation data set only has 16 variables and 259 observations. Evaluation data set is used to evaluate the performance of the final model and that is why it does not have WINS variable given. All variables are integer data type. 

The response variable will be TEAM_WINS and the rest variables are predictors. Let's do column names comparison for both dataset to make sure same variable exist on both dataset except WINS variable.

```{r}

list(training,evaluation) %>% compare_df_cols() %>% `colnames<-`(c('Column Name', 'Training', 'Evaluation'))

```

Statistics summary and visualization for training data set

```{r}

training %>% select(-INDEX) %>% describe()

```

From the statistic summary table above, we can see there are outliers exist in many variables, many of them are heavily skewed, and bimodal. We can visualize this clearly in below charts. We're going to deal with these issues later in data preparation section.

```{r}

training %>% select(-INDEX) %>% plot_histogram()
training %>% select(-INDEX) %>% plot_density()
training %>% select(-INDEX) %>% gather(variable, value) %>% plot_boxplot(by='variable',geom_boxplot_args = list("outlier.color" = "red"))

```

In additional to above analysis, below chart is showing percentage of missing values for each variable and the table is showing the number of records have zero value. we can see quite a few of the variables have missing values and contain zero value. The TEAM_BATTING_HBP variable can be dropped as it has 92% missing data.

For the missing value, we'll impute the data and for the zero value we will do replacement with mean value for that variable.

```{r}

training %>% select(-INDEX) %>% plot_missing()

training %>% select(-INDEX) %>% gather(variable, value) %>% filter(value == 0) %>% 
  group_by(variable) %>% tally() %>% mutate(percent = n / nrow(training) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>% rename(`Variable With Zeros` = variable, 
                              `Number of Records` = n,
                              `Share of Total` = percent)

```

# Data Preparation

In below chunk of codes, we're dropping INDEX and TEAM_BATTING_HBP variable as these two variables are useless. Also, we're imputing the missing value using the MICE package and then replace zero value with mean.
```{r}

# Drop the index and bat_hbp variable
train_df <- training %>% select(-c(INDEX, TEAM_BATTING_HBP))

# Use the mice package to impute the missing values
imputed_data <- mice(train_df, m=5, maxit=30, method='pmm', seed=321)

# Select complete imputed data set number 2
complete_train_data <- complete(imputed_data,2)

# Replace zero value with mean
complete_train_data$TEAM_BATTING_SO[complete_train_data$TEAM_BATTING_SO==0] <- as.integer(mean(complete_train_data[["TEAM_BATTING_SO"]]))
complete_train_data$TEAM_PITCHING_SO[complete_train_data$TEAM_PITCHING_SO==0] <- as.integer(mean(complete_train_data[["TEAM_PITCHING_SO"]]))
complete_train_data$TEAM_BATTING_HR[complete_train_data$TEAM_BATTING_HR==0] <- as.integer(mean(complete_train_data[["TEAM_BATTING_HR"]]))
complete_train_data$TEAM_PITCHING_HR[complete_train_data$TEAM_PITCHING_HR==0] <- as.integer(mean(complete_train_data[["TEAM_PITCHING_HR"]]))
complete_train_data$TEAM_BASERUN_SB[complete_train_data$TEAM_BASERUN_SB==0] <- as.integer(mean(complete_train_data[["TEAM_BASERUN_SB"]]))
complete_train_data$TEAM_BATTING_3B[complete_train_data$TEAM_BATTING_3B==0] <- as.integer(mean(complete_train_data[["TEAM_BATTING_3B"]]))
complete_train_data$TARGET_WINS[complete_train_data$TARGET_WINS==0] <- as.integer(mean(complete_train_data[["TARGET_WINS"]]))
complete_train_data$TEAM_BASERUN_CS[complete_train_data$TEAM_BASERUN_CS==0] <- as.integer(mean(complete_train_data[["TEAM_BASERUN_CS"]]))
complete_train_data$TEAM_BATTING_BB[complete_train_data$TEAM_BATTING_BB==0] <- as.integer(mean(complete_train_data[["TEAM_BATTING_BB"]]))
complete_train_data$TEAM_PITCHING_BB[complete_train_data$TEAM_PITCHING_BB==0] <- as.integer(mean(complete_train_data[["TEAM_PITCHING_BB"]]))

```

Now, we want to remove outliers from the dataset.

```{r}

# Remove outlier for TEAM_BATTING_H variable
Q <- quantile(complete_train_data$TEAM_BATTING_H, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_BATTING_H)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_BATTING_H > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_BATTING_H < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_BATTING_2B variable
Q <- quantile(complete_train_data$TEAM_BATTING_2B, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_BATTING_2B)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_BATTING_2B > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_BATTING_2B < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_BATTING_3B variable
Q <- quantile(complete_train_data$TEAM_BATTING_3B, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_BATTING_3B)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_BATTING_3B > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_BATTING_3B < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_BATTING_HR variable
Q <- quantile(complete_train_data$TEAM_BATTING_HR, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_BATTING_HR)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_BATTING_HR > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_BATTING_HR < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_BATTING_BB variable
Q <- quantile(complete_train_data$TEAM_BATTING_BB, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_BATTING_BB)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_BATTING_BB > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_BATTING_BB < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_BATTING_SO variable
Q <- quantile(complete_train_data$TEAM_BATTING_SO, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_BATTING_SO)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_BATTING_SO > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_BATTING_SO < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_BASERUN_SB variable
Q <- quantile(complete_train_data$TEAM_BASERUN_SB, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_BASERUN_SB)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_BASERUN_SB > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_BASERUN_SB < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_BASERUN_CS variable
Q <- quantile(complete_train_data$TEAM_BASERUN_CS, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_BASERUN_CS)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_BASERUN_CS > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_BASERUN_CS < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_PITCHING_H variable
Q <- quantile(complete_train_data$TEAM_PITCHING_H, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_PITCHING_H)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_PITCHING_H > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_PITCHING_H < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_PITCHING_HR variable
Q <- quantile(complete_train_data$TEAM_PITCHING_HR, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_PITCHING_HR)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_PITCHING_HR > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_PITCHING_HR < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_PITCHING_BB variable
Q <- quantile(complete_train_data$TEAM_PITCHING_BB, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_PITCHING_BB)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_PITCHING_BB > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_PITCHING_BB < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_PITCHING_SO variable
Q <- quantile(complete_train_data$TEAM_PITCHING_SO, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_PITCHING_SO)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_PITCHING_SO > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_PITCHING_SO < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_FIELDING_E variable
Q <- quantile(complete_train_data$TEAM_FIELDING_E, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_FIELDING_E)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_FIELDING_E > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_FIELDING_E < (Q[2]+1.5*iqr))

# Remove outlier for TEAM_FIELDING_DP variable
Q <- quantile(complete_train_data$TEAM_FIELDING_DP, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(complete_train_data$TEAM_FIELDING_DP)
complete_train_data <- subset(complete_train_data, complete_train_data$TEAM_FIELDING_DP > (Q[1] - 1.5*iqr) & complete_train_data$TEAM_FIELDING_DP < (Q[2]+1.5*iqr))

complete_train_data %>% gather(variable, value) %>% plot_boxplot(by='variable',geom_boxplot_args = list("outlier.color" = "red"))

complete_train_data %>% plot_histogram()

```

Before we step in to build models, we should check the sign of collinearity among the predictors by looking at the pairwise correlations. From the figure below, we can see there are several large pairwise correlations between predictors as highlighted in red or summarized as below. When building models, we can consider to pick one predictor over the other for each pair because one of them can represent the other. 

- TEAM_PITCHING_SO : TEAM_BATTING_SO
- TEAM_PITCHING_BB : TEAM_BATTING_BB
- TEAM_PITCHING_HR : TEAM_BATTING_HR
- TEAM_PITCHING_H : TEAM_BATTING_H

```{r}

plot_correlation(complete_train_data)

```

# Build Models

```{r saturated model}

# Saturated linear regression model
r.squared <- c()
sat_lm <- lm(TARGET_WINS ~ ., data = complete_train_data)
summary(sat_lm)
r.squared['full'] = summary(sat_lm)$adj.r.squared

```

We can use backward elimination method to eliminate variables do not contribute to the regression equation.

From the summary, we can see these 4 variables: TEAM_BATTING_H, TEAM_BASERUN_CS, TEAM_PITCHING_HR, TEAM_PITCHING_SO which p-value are statistically insignificant. 

```{r 1st model}

# Test whether pitch_hr, picth_bb, brun_cs can be dropped
lm <- lm(TARGET_WINS ~ ., data = subset(complete_train_data, select=-c(TEAM_BATTING_H,TEAM_BASERUN_CS,TEAM_PITCHING_HR,TEAM_PITCHING_SO)))
anova(lm, sat_lm)

```

All the 4 variables can be dropped as its p-value of F-test is 0.1214. The null hypothesis cannot be rejected.

```{r permutation test}

# Normal theory-based p-value
lms <- summary(lm)
format.pval(1 - pf(lms$fstatistic[1],lms$fstatistic[2],lms$fstatistic[3]))

# Permutation test based p-value
nreps <- 5000
set.seed(123)
fstats <- numeric(nreps)
for(i in 1:nreps){
  lmods <- lm(sample(TARGET_WINS) ~ ., data = subset(complete_train_data,select=-c(TEAM_BATTING_H,TEAM_BASERUN_CS,TEAM_PITCHING_HR,TEAM_PITCHING_SO)))
  fstats[i] <- summary(lmods)$fstat[1]
}

format.pval(mean(fstats > lms$fstat[1]))

```

We can see the permutation test result similar to the normal theory-based value of 2.22e-16. Also, the p-value is less than the significance level, we can reject the null hypothesis. This indicates the imputed training data set provides sufficient evidence to conclude the regression model fits the data better than the model with no predictor variables.

Evaluate the first model through residuals plot and QQ plot and also Shapiro-Wilk normality test. We can see this model meets the 3 conditions (linearity, nearly normal residuals, constant variability) of least squares regression and also the Shapiro-Wilk normality test is greater than 0.05 indicating normality of the data.

```{r}

par(mfrow=c(2,2))

# Residuals plot
plot(lm$fitted.values, lm$residuals, 
     xlab='Fitted Values', ylab='Residuals')
abline(h = 0, lty = 3, col="blue")
abline(h = 40, lty = 3, col="red")
abline(h = -40, lty = 3, col="red")

# Histogram plot
hist(lm$residuals, xlab="Residuals")

# QQ plot
qqnorm(lm$residuals)
qqline(lm$residuals)


# Check for nonconstant variances: H0: Constant Variance, HA: Nonconstant Variance
# P-value of .011 <.05, reject null hypothesis for alternative indicating Nonconstant Variance

summary (lm (abs (lm$residuals) ~ lm$fitted.values))

# Test Residuals for Normality
# H0: Residuals are normally distributed, HA: Residuals are Not Normally Distributed
# P-value <.05 so we reject the null hypothesis in favor of the alternative indicating
# Residuals are not normally distributed

shapiro.test(lm$residuals)


```

Check the linear model summary to determine which variables can also be dropped.

```{r}

summary(lm)
r.squared['lm'] = summary(lm)$adj.r.squared

```
